{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4552b4",
   "metadata": {},
   "source": [
    "This function will take a list of dictionaries and convert that structure into an initial neural network with He weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3040e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_layers(nn_architecture, seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    params_values = {}\n",
    "\n",
    "    # Take the list of dictionaries and construct a NN with randomly assigned weights and bias vectors.\n",
    "    # Layers are given small random numbers as their inital state so as not to encounter the breaking symmetry problem if all given the same values and it provides a good enough starting point.\n",
    "    # Using small values increases the efficiency of the algorithm during the first iterations.\n",
    "    for i in range(1, len(nn_architecture)):\n",
    "        layer_input_size = nn_architecture[i-1][\"input_nodes\"]\n",
    "\n",
    "        if (i == len(nn_architecture)-1):\n",
    "            layer_output_size = 10\n",
    "        else:\n",
    "            layer_output_size = nn_architecture[i][\"input_nodes\"]\n",
    "        \n",
    "        # Initialize layer node's weights using He Weight Initialization. This is suitable as we are using ReLU activation functions.\n",
    "        # Source: https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html\n",
    "        # This stage is important as incorrect weight initialization can lead to vanishing/exploding gradients.\n",
    "        params_values['W' + str(i)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * np.sqrt(2.0 / layer_input_size)\n",
    "        \n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0094a8",
   "metadata": {},
   "source": [
    "Sigmoid, ReLU, and Softmax activation functions for both forward popagation and backward (deriative) propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81d12bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function - Sigmoid - Forward Propagation\n",
    "def sigmoid(Z):\n",
    "    return 1.0/(1.0+np.exp(-Z))\n",
    "\n",
    "# Activation Function - ReLU - Forward Propagation\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "# Activation Function - Softmax - Forward Propagation - Avoids overflow or underflow due to floating point instability.\n",
    "def softmax(Z):\n",
    "    e = np.exp(Z - np.max(Z))\n",
    "    return e / np.sum(e, axis = 0)\n",
    "\n",
    "\n",
    "# Activation Function - Softmax - Backward Propagation - Avoids overflow or underflow due to floating point instability.\n",
    "def softmax_backward(Z):\n",
    "    smax = softmax(Z)\n",
    "    return smax * (1 - smax)\n",
    "\n",
    "# Activation Function - Sigmoid - Backward Propagation\n",
    "def sigmoid_backward(Z):\n",
    "    sig = sigmoid(Z)\n",
    "    # return (np.exp(-Z))/((np.exp(-Z)+1)**2)\n",
    "    return sig * (1-sig)\n",
    "\n",
    "# Activation Function - ReLU - Backward Propagation\n",
    "def relu_backward(Z):\n",
    "    dZ = np.array(Z, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    dZ[Z > 0] = 1\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59484d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.02508785, -0.0069834 ,  0.03271321, ..., -0.06744508,\n",
      "         0.01920289,  0.03083924],\n",
      "       [ 0.02827369,  0.05458767,  0.04211943, ..., -0.01420919,\n",
      "         0.00338354,  0.02605887],\n",
      "       [-0.07892048, -0.0267212 ,  0.04011642, ..., -0.01979963,\n",
      "         0.05353487,  0.03116351],\n",
      "       ...,\n",
      "       [-0.01520984, -0.02996139,  0.07425277, ...,  0.08622519,\n",
      "        -0.0071112 ,  0.04780936],\n",
      "       [ 0.04152834,  0.03256375, -0.01150627, ..., -0.08783793,\n",
      "        -0.01636675, -0.0307535 ],\n",
      "       [ 0.08485768, -0.00966599, -0.03247982, ...,  0.06209712,\n",
      "         0.08136642,  0.07013439]]), 'W2': array([[-0.01002943,  0.095423  ,  0.13284665, ...,  0.13962699,\n",
      "         0.12200584,  0.08994849],\n",
      "       [-0.04016608, -0.00100266, -0.15665874, ..., -0.08438804,\n",
      "        -0.06913039, -0.23780935],\n",
      "       [-0.01403963, -0.08706934, -0.08593643, ..., -0.07263895,\n",
      "        -0.04598236,  0.10144271],\n",
      "       ...,\n",
      "       [-0.15393538,  0.04641696, -0.28507112, ...,  0.00053725,\n",
      "        -0.00198338, -0.08428685],\n",
      "       [ 0.03490654,  0.09318763,  0.28860436, ..., -0.02857815,\n",
      "         0.19675727, -0.16956257],\n",
      "       [-0.18590272, -0.26550699, -0.23370946, ..., -0.0706754 ,\n",
      "         0.01092492,  0.01434675]])}\n"
     ]
    }
   ],
   "source": [
    "# Setup the initialization array which outlines the architecture of the NN and will be passed into the program to generate the appropraite NN.\n",
    "nn_architecture = [\n",
    "    {\"input_nodes\": 784, \"activation\": \"relu\"},\n",
    "    {\"input_nodes\": 128, \"activation\": \"relu\"},\n",
    "    {\"input_nodes\": 10, \"activation\": \"softmax\"},\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('fashion-mnist_test.csv')\n",
    "df_train = pd.read_csv('fashion-mnist_train.csv')\n",
    "\n",
    "y_train = np.array(df_train['label'].to_numpy())\n",
    "df_train = df_train.drop('label', 1)\n",
    "\n",
    "# Normalziing the pixel data\n",
    "X_train = np.array((df_train.to_numpy() / 255).astype('float32'))\n",
    "\n",
    "y_test = np.array(df_test['label'].to_numpy())\n",
    "df_test = df_test.drop('label', 1)\n",
    "\n",
    "# Normalziing the pixel data\n",
    "X_test = np.array((df_test.to_numpy() / 255).astype('float32'))\n",
    "\n",
    "# One Hot Encoding the labels\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "# Create neural network based on specified architecture with initial weights.\n",
    "params_values = init_layers(nn_architecture, 42)\n",
    "print(params_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d63ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
