{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4552b4",
   "metadata": {},
   "source": [
    "This function will take a list of dictionaries and convert that structure into an initial neural network with He weight initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3040e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_layers(nn_architecture, seed = 42):\n",
    "    np.random.seed(seed)\n",
    "    params_values = {}\n",
    "\n",
    "    # Take the list of dictionaries and construct a NN with randomly assigned weights and bias vectors.\n",
    "    # Layers are given small random numbers as their inital state so as not to encounter the breaking symmetry problem if all given the same values and it provides a good enough starting point.\n",
    "    # Using small values increases the efficiency of the algorithm during the first iterations.\n",
    "    for i in range(1, len(nn_architecture)):\n",
    "        layer_input_size = nn_architecture[i-1][\"input_nodes\"]\n",
    "\n",
    "        if (i == len(nn_architecture)-1):\n",
    "            layer_output_size = 10\n",
    "        else:\n",
    "            layer_output_size = nn_architecture[i][\"input_nodes\"]\n",
    "        \n",
    "        # Initialize layer node's weights using He Weight Initialization. This is suitable as we are using ReLU activation functions.\n",
    "        # Source: https://datascience-enthusiast.com/DL/Improving-DeepNeural-Networks-Initialization.html\n",
    "        # This stage is important as incorrect weight initialization can lead to vanishing/exploding gradients.\n",
    "        params_values['W' + str(i)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * np.sqrt(2.0 / layer_input_size)\n",
    "        \n",
    "        \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0094a8",
   "metadata": {},
   "source": [
    "Sigmoid, ReLU, and Softmax activation functions for both forward popagation and backward (deriative) propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81d12bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Function - Sigmoid - Forward Propagation\n",
    "def sigmoid(Z):\n",
    "    return 1.0/(1.0+np.exp(-Z))\n",
    "\n",
    "# Activation Function - ReLU - Forward Propagation\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "# Activation Function - Softmax - Forward Propagation - Avoids overflow or underflow due to floating point instability.\n",
    "def softmax(Z):\n",
    "    e = np.exp(Z - np.max(Z))\n",
    "    return e / np.sum(e, axis = 0)\n",
    "\n",
    "\n",
    "# Activation Function - Softmax - Backward Propagation - Avoids overflow or underflow due to floating point instability.\n",
    "def softmax_backward(Z):\n",
    "    smax = softmax(Z)\n",
    "    return smax * (1 - smax)\n",
    "\n",
    "# Activation Function - Sigmoid - Backward Propagation\n",
    "def sigmoid_backward(Z):\n",
    "    sig = sigmoid(Z)\n",
    "    # return (np.exp(-Z))/((np.exp(-Z)+1)**2)\n",
    "    return sig * (1-sig)\n",
    "\n",
    "# Activation Function - ReLU - Backward Propagation\n",
    "def relu_backward(Z):\n",
    "    dZ = np.array(Z, copy = True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    dZ[Z > 0] = 1\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a22308",
   "metadata": {},
   "source": [
    "Forward Propagation is split into two functions, single layer step-forward and entire NN step forward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eebdbf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, activation=\"relu\"):\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation == \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    elif activation == \"softmax\":\n",
    "        activation_func = softmax\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    # Compute error.\n",
    "    Z_curr = np.dot(W_curr, A_prev)\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "# X - Input Matrix\n",
    "# Will perform a full forward step propagation and organize all intermdeite values returned from each step forward of a layer.\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    memory[\"A0\"] = A_curr\n",
    "    \n",
    "    for idx in range(len(nn_architecture)):\n",
    "        # Don't evaluate the output layer\n",
    "        if (idx == len(nn_architecture)-1): continue\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # Get current layer's activation function\n",
    "        activ_function_curr = nn_architecture[layer_idx][\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(layer_idx)] = A_curr\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "    \n",
    "    # Return A value of the output layer and computed values of all layers.\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d8801",
   "metadata": {},
   "source": [
    "Backward Propagation is split into two functions, single layer step-backward and entire NN step backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e80e5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(y, W_curr, Z_curr, A_prev, Y, is_output_layer, error, activation=\"relu\"):\n",
    "    if activation == \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation == \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    elif activation == \"softmax\":\n",
    "        backward_activation_func = softmax_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    dZ_curr = backward_activation_func(Z_curr)\n",
    "    error_cal = None\n",
    "\n",
    "    # Error of the output layer\n",
    "    if is_output_layer:\n",
    "        error_cal = 2 * (y - Y) / y.shape[0] * dZ_curr\n",
    "    # Error of the hidden layers\n",
    "    else:\n",
    "        error_cal = np.dot(W_curr.T, error) * dZ_curr\n",
    "    \n",
    "    dW_curr = np.dot(error_cal, A_prev.T)\n",
    "    return dW_curr, error_cal\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    error = 0\n",
    "    \n",
    "    for layer_idx_curr, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # Don't calculate the weight's gradient values for the input layer.\n",
    "        if (layer_idx_curr == 0): continue\n",
    "        \n",
    "        layer_idx_prev = layer_idx_curr - 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr+1)] if layer_idx_curr != len(nn_architecture)-1 else None\n",
    "        \n",
    "        is_output_layer = True if layer_idx_curr == len(nn_architecture)-1 else False\n",
    "        dW_curr, error = single_layer_backward_propagation(\n",
    "            Y_hat, W_curr, Z_curr, A_prev, Y, is_output_layer, error, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d2ab34",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "376e4e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent optimization.\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        if (layer_idx == len(nn_architecture)-1): continue\n",
    "        params_values[\"W\" + str(layer_idx+1)] -= learning_rate * grads_values[\"dW\" + str(layer_idx+1)]\n",
    "\n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32d1ff9",
   "metadata": {},
   "source": [
    "For a dataset, which is a multi-class clasification problem, it is best suited to use a softmax activation function for the output layer as well as cross entropy to compute the loss value for a particular predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b78178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function - Calculates categorical cross entropy\n",
    "# Cross entropy will provide a score for each value passed in. This score represents the distance from the actual value/label.\n",
    "# As this is logarithmic, small differences are given small scores and large differences are given enormous scores.\n",
    "# These scores are then used to penalize the probability.\n",
    "\n",
    "# An alternative, more efficeint function of this idea is scikit-learn's log_loss() function.\n",
    "def get_metrics(X_test, y_test, params_values, nn_architecture):\n",
    "    predictions_accuracy = np.array([])\n",
    "    predictions_cost = np.array([])\n",
    "    \n",
    "    for X, y in zip(X_test, y_test):\n",
    "        output, memory = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        predictions_accuracy = np.append(predictions_accuracy, np.argmax(output) == np.argmax(y))\n",
    "\n",
    "        for idx, val in enumerate(y):\n",
    "            # A very small value of 1e-15 is added to the predicted probability to prevent ever calculating the log of 0. \n",
    "            predictions_cost = np.append(predictions_cost, val * np.log(1e-15 + output[idx]))\n",
    "            \n",
    "    return np.mean(predictions_accuracy), -np.sum(predictions_cost) / y_test.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2d29fd",
   "metadata": {},
   "source": [
    "This will take the target and features and create a mini-batch of the total data based on batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b34afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from this post: https://stackoverflow.com/a/54647545/10439539\n",
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert inputs.shape[0] == targets.shape[0]\n",
    "    if shuffle:\n",
    "        indices = np.arange(inputs.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, inputs.shape[0], batchsize):\n",
    "        end_idx = min(start_idx + batchsize, inputs.shape[0])\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:end_idx]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, end_idx)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77ac68",
   "metadata": {},
   "source": [
    "Function to train the NN using the forward and backward propagation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531801d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train(X_train, y_train, X_test, y_test, nn_architecture, batch_size, epochs, learning_rate, seed):\n",
    "    # Create neural network based on specified architecture with initial weights.\n",
    "    params_values = init_layers(nn_architecture, seed)\n",
    "    metrics = [[], []]\n",
    "    \n",
    "    # Timer for time metrics.\n",
    "    start_time = time.time()\n",
    "    for i in range(epochs):\n",
    "        for batch in iterate_minibatches(X_train, y_train, batch_size, seed, shuffle=True):\n",
    "            X_batch, y_batch = batch\n",
    "            Y_hat, memory = full_forward_propagation(X_batch.T, params_values, nn_architecture)\n",
    "            \n",
    "            grads_values = full_backward_propagation(Y_hat, y_batch.T, memory, params_values, nn_architecture)\n",
    "            params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "\n",
    "        accuracy, loss = get_metrics(X_test, y_test, params_values, nn_architecture)\n",
    "        metrics[0].append(accuracy)\n",
    "        metrics[1].append(loss)\n",
    "\n",
    "        print('Epoch: {0}, Total Time Spent: {1:.2f}s, Accuracy: {2:.2f}%, Loss: {3:.3f}'.format(\n",
    "                i+1, time.time() - start_time, accuracy * 100, loss\n",
    "            ))\n",
    "        \n",
    "    return params_values, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59484d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emre\\AppData\\Local\\Temp/ipykernel_4304/1881147201.py:14: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df_train = df_train.drop('label', 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.02508785, -0.0069834 ,  0.03271321, ..., -0.06744508,\n",
      "         0.01920289,  0.03083924],\n",
      "       [ 0.02827369,  0.05458767,  0.04211943, ..., -0.01420919,\n",
      "         0.00338354,  0.02605887],\n",
      "       [-0.07892048, -0.0267212 ,  0.04011642, ..., -0.01979963,\n",
      "         0.05353487,  0.03116351],\n",
      "       ...,\n",
      "       [-0.01520984, -0.02996139,  0.07425277, ...,  0.08622519,\n",
      "        -0.0071112 ,  0.04780936],\n",
      "       [ 0.04152834,  0.03256375, -0.01150627, ..., -0.08783793,\n",
      "        -0.01636675, -0.0307535 ],\n",
      "       [ 0.08485768, -0.00966599, -0.03247982, ...,  0.06209712,\n",
      "         0.08136642,  0.07013439]]), 'W2': array([[-0.01002943,  0.095423  ,  0.13284665, ...,  0.13962699,\n",
      "         0.12200584,  0.08994849],\n",
      "       [-0.04016608, -0.00100266, -0.15665874, ..., -0.08438804,\n",
      "        -0.06913039, -0.23780935],\n",
      "       [-0.01403963, -0.08706934, -0.08593643, ..., -0.07263895,\n",
      "        -0.04598236,  0.10144271],\n",
      "       ...,\n",
      "       [-0.15393538,  0.04641696, -0.28507112, ...,  0.00053725,\n",
      "        -0.00198338, -0.08428685],\n",
      "       [ 0.03490654,  0.09318763,  0.28860436, ..., -0.02857815,\n",
      "         0.19675727, -0.16956257],\n",
      "       [-0.18590272, -0.26550699, -0.23370946, ..., -0.0706754 ,\n",
      "         0.01092492,  0.01434675]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Emre\\AppData\\Local\\Temp/ipykernel_4304/1881147201.py:20: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df_test = df_test.drop('label', 1)\n"
     ]
    }
   ],
   "source": [
    "# Setup the initialization array which outlines the architecture of the NN and will be passed into the program to generate the appropraite NN.\n",
    "nn_architecture = [\n",
    "    {\"input_nodes\": 784, \"activation\": \"relu\"},\n",
    "    {\"input_nodes\": 128, \"activation\": \"relu\"},\n",
    "    {\"input_nodes\": 64, \"activation\": \"relu\"},\n",
    "    {\"input_nodes\": 10, \"activation\": \"softmax\"},\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv('./Dataset/fashion-mnist_test.csv')\n",
    "df_train = pd.read_csv('./Dataset/fashion-mnist_train.csv')\n",
    "\n",
    "y_train = np.array(df_train['label'].to_numpy())\n",
    "df_train = df_train.drop('label', 1)\n",
    "\n",
    "# Normalziing the pixel data\n",
    "X_train = np.array((df_train.to_numpy() / 255).astype('float32'))\n",
    "\n",
    "y_test = np.array(df_test['label'].to_numpy())\n",
    "df_test = df_test.drop('label', 1)\n",
    "\n",
    "# Normalziing the pixel data\n",
    "X_test = np.array((df_test.to_numpy() / 255).astype('float32'))\n",
    "\n",
    "# One Hot Encoding the labels\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "params_values, metrics_history = train(X_train, y_train, X_test, y_test, nn_architecture, 128, 10, 0.005, 42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
