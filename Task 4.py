# -*- coding: utf-8 -*-
"""Task 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZCL7iXV4P-kyBLXCifQtTrJlDw4hwmn

# Programming and Mathematics for AI: Task 4

# Part 1: Implement a Neural Network (Baseline)

Imports
"""

# Commented out IPython magic to ensure Python compatibility.
# NumPy
import numpy as np

# PyTorch - Open-source machine learning framework
import torch
import torch.nn as nn # Neural network
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data.sampler import SubsetRandomSampler

#Torchvision - Package consisting of popular datasets, model architectures, and common image transformations for computer vision
import torchvision
# from torchvision import datasets # Possibly remove
import torchvision.transforms as transforms
from torchvision.datasets import CIFAR10 # Dataset
from torchvision.transforms import ToTensor
from torchvision.utils import make_grid
from torch.utils.data.dataloader import DataLoader
from torch.utils.data import random_split

# Matplotlib - Visualising data
import matplotlib.pyplot as plt
# %matplotlib inline

"""Downloading CIFAR-10 dataset, and examining its contents. 50,000 images are intended for training, and 10,000 for testing.  We will split this further by including a validation set of 5,000 images from the training set."""

training_data = CIFAR10(root = 'data/', download = True, transform = ToTensor())

testing_data = CIFAR10(root = 'data/', train = False, transform = ToTensor())

classes = training_data.classes
classes

training_data_size = len(training_data)
training_data_size

testing_data_size = len(testing_data)
testing_data_size

classes_count = len(training_data.classes)
classes_count

# Image shape
image, label = training_data[0]
image_shape = image.shape
image_shape

"""Further splitting data to get a validation set.  Using a seed value to ensure the same validation set is always used."""

# Validation data size
validation_size = 5000
# New training data size
training_size = len(training_data) - validation_size
# Random seed for reproducibility
torch.manual_seed(25)

"""Using random split on training data to allocate 5,000 images to validation."""

training_data, validation_data = random_split(training_data, [training_size, validation_size])

# Final datasets
print(len(training_data)) # Data used to fit the model
print(len(validation_data)) # Data used to fine-tune the model hyperparameters
print(len(testing_data)) # Data used to provide an unbiased evaluation of a final NN

"""Baseline Parameters

Batch size defines the number of samples that will be propagated through the neural network, number of epochs determines training times, optimiser function has several options.  Learning rate describes the rate at which the model optimises.
"""

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

"""Data loaders for training, validation, testing sets.  We used shuffle for the training data loader so that batches are different per epoch during training.  This boosts generalisation. 2 workers are used, as recommended by colab."""

training_loader = DataLoader(training_data, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)

validation_loader = DataLoader(validation_data, batch_size*2, num_workers, pin_memory=True)

testing_loader = DataLoader(testing_data, batch_size*2, num_workers, pin_memory=True)

"""# Building the Neural Network

Function to determine model accuracy
"""

def determine_accuracy(outputs, labels):
    """Determine accuracy via predictions"""
    _, preds = torch.max(outputs, dim=1)
    return torch.tensor(torch.sum(preds == labels).item() / len(preds))

class NeuralNetwork(nn.Module):

    def train_batch(self, batch):
        """Training batch: making predictions and determining loss"""

        batch_images, batch_labels = batch

        # Make predictions
        out = self(batch_images)

        # Determine batch loss
        batch_loss = F.cross_entropy(out, batch_labels)
        return batch_loss
    
    def validate_batch(self, batch):
        """Validating batch: making predictions, determining loss and accuracy"""

        batch_images, batch_labels = batch

        # Make predictions
        out = self(batch_images)

        # Determine loss
        batch_loss = F.cross_entropy(out, batch_labels)

        # Calculate accuracy
        accuracy = determine_accuracy(out, batch_labels)
        return {'validation_loss': batch_loss.detach(), 'validation_accuracy': accuracy}
        
    def validate_epoch(self, outputs):
        """Validate batches at the end of epoch"""

        # Losses across epochs
        epoch_losses = [x['validation_loss'] for x in outputs]

        # Combining losses using mean
        combined_epoch_loss = torch.stack(epoch_losses).mean()

        # Accuracies across epochs
        epoch_accs = [x['validation_accuracy'] for x in outputs]

        # Combining accuracies using mean
        combined_epoch_acc = torch.stack(epoch_accs).mean()

        return {'validation_loss': combined_epoch_loss.item(), 'validation_accuracy': combined_epoch_acc.item()}
    
    def print_epoch(self, epoch, result):
        """Print epoch number, validation loss and accuracy at the end of each epoch"""

        print("Epoch {} | validation_loss: {:.4f} | validation_accuracy: {:.4f}".format(epoch, result['validation_loss'], result['validation_accuracy']))

"""Function to evaluate model using model's internal functions"""

def validate_model(model, validation_loader):
    """Model validation"""

    outputs = [model.validate_batch(batch) for batch in validation_loader]
    return model.validate_epoch(outputs)

"""Function to train model using evaluation function above, and model's internal training functions."""

def fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function):
    """Model training and validation"""

    # Model history
    model_history = []

    # Model optimiser
    model_optimiser = optimiser_function(model.parameters(), learning_rate)

    for epoch in range(epochs):

        # Training
        for batch in training_loader:

            loss = model.train_batch(batch)
            loss.backward()

            model_optimiser.step()
            model_optimiser.zero_grad()

        # Validation
        result = validate_model(model, validation_loader)
        model.print_epoch(epoch, result)
        model_history.append(result)

    return model_history

"""Checking the use of a dedicated GPU: In our case we mostly used CPUs."""

def get_device():
    """Use dedicated GPU if available, else use CPU"""
    if torch.cuda.is_available():
        return torch.device('cuda')
    else:
        return torch.device('cpu')

device = get_device()
print(device)

def move_to_device(data, device):
    """Move data to GPU/CPU"""
    if isinstance(data, (list,tuple)):
        # Using recursion if required
        return [move_to_device(x, device) for x in data]

    return data.to(device, non_blocking=True)

"""Function to plot loss across model history."""

def plot_losses(history):
    """Plot loss across model history"""

    losses = [x['validation_loss'] for x in history]

    # Plotting using pyplot
    plt.plot(losses, '-x')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Loss against epochs');

"""Function to plot accuracy across model history."""

def plot_accuracies(history):
    """Plot accuracy across model history"""

    accuracies = [x['validation_accuracy'] for x in history]

    # Plotting using pyplot
    plt.plot(accuracies, '-x')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Accuracy against epochs');

"""Data Loaders"""

class DeviceDataLoader():
    """Move data to a device"""

    def __init__(self, data_loader, device):
        self.data_loader = data_loader
        self.device = device
      
    def __len__(self):
        """Batch count"""

        return len(self.data_loader)
        
    def __iter__(self):
        """Yield a batch of data after moving it to device"""
        
        for b in self.data_loader: 
            yield move_to_device(b, self.device)

training_loader = DeviceDataLoader(training_loader, device)
validation_loader = DeviceDataLoader(validation_loader, device)
testing_loader = DeviceDataLoader(testing_loader, device)

"""# Training the Neural Network"""

# Input and output

input_size = 3*32*32

output_size = 10

class Model_1(NeuralNetwork):
    def __init__(self):

        super().__init__()
        self.linear1 = nn.Linear(input_size, 256)
        self.linear2 = nn.Linear(256, 128)
        self.linear3 = nn.Linear(128, output_size)
        
    def forward(self, x):

        # Flatten images into vectors
        x = x.view(x.size(0), -1)

        # Apply layers & activation functions
        x = self.linear1(x)
        x = F.relu(x)
        x = self.linear2(x)
        x = F.relu(x)
        x = self.linear3(x)
        
        return x

"""Move model to device"""

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

"""Initial results"""

plot_losses(history)

plot_accuracies(history)

learning_rate = 1e-2

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

learning_rate = 1e-3

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

"""Plotting losses across all epochs"""

plot_losses(history)

"""Plotting accuracies across all epochs"""

plot_accuracies(history)

"""Evaluating model using the testing data and validate_model function"""

validate_model(model, testing_loader)

# ...

"""# Part 2: Propose Improvements

# Adding a dropout layer
"""

class Model_dropout(NeuralNetwork):
    def __init__(self):

        super().__init__()
        self.linear1 = nn.Linear(input_size, 256)
        self.linear2 = nn.Linear(256, 128)
        self.linear3 = nn.Linear(128, output_size)

        # Dropout layer: probability set to 0.25
        self.dropout = nn.Dropout(0.25)
        
    def forward(self, x):

        # Flatten images into vectors
        x = x.view(x.size(0), -1)

        # Apply layers & activation functions
        x = self.linear1(x)
        x = F.relu(x)
        x = self.linear2(x)
        x = F.relu(x)
        # Applying dropout
        x = self.dropout(x)
        x = self.linear3(x)
        
        return x

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_dropout(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

"""Trying a different dropout probability of 0.15:"""

class Model_dropout(NeuralNetwork):
    def __init__(self):

        super().__init__()
        self.linear1 = nn.Linear(input_size, 256)
        self.linear2 = nn.Linear(256, 128)
        self.linear3 = nn.Linear(128, output_size)

        # Dropout layer: probability set to 0.15
        self.dropout = nn.Dropout(0.15)
        
    def forward(self, x):

        # Flatten images into vectors
        x = x.view(x.size(0), -1)

        # Apply layers & activation functions
        x = self.linear1(x)
        x = F.relu(x)
        x = self.linear2(x)
        x = F.relu(x)
        # Applying dropout
        x = self.dropout(x)
        x = self.linear3(x)
        
        return x

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_dropout(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

"""# Improvement Attempt: L2 Regularisation

L2 regularisation reduces overfitting by shrinking parameter estimates, which simplifies the model.  This is facilitated via the weight_decay parameter in the optimiser.
"""

# Edited fit_model function to include L2 (weight_decay)

def fit_model_L2(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function, weight_decay):
    """Model training and validation"""

    # Model history
    model_history = []

    # Model optimiser
    model_optimiser = optimiser_function(model.parameters(), learning_rate, weight_decay) # Edited here to include weight_decay

    for epoch in range(epochs):

        # Training
        for batch in training_loader:

            loss = model.train_batch(batch)
            loss.backward()

            model_optimiser.step()
            model_optimiser.zero_grad()

        # Validation
        result = validate_model(model, validation_loader) 
        model.print_epoch(epoch, result)
        model_history.append(result)

    return model_history

"""Running with weight decay = 1e-5"""

# Running baseline with weight decay added to optimiser.

weight_decay = 1e-5

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model_L2(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function, weight_decay)

plot_losses(history)

plot_accuracies(history)

"""Running with weight decay = 1e-4"""

# Running baseline with weight decay added to optimiser.

weight_decay = 1e-4

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model_L2(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function, weight_decay)

plot_losses(history)

plot_accuracies(history)

"""# Part 3: Evaluating Parameters

Running baseline model while varying number of epochs: 10, 25, 50
"""

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

epochs = 25
model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

epochs = 50
model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

"""Varying batch size: 64, 128, 256"""

# Baseline: 128

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

training_loader = DataLoader(training_data, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
validation_loader = DataLoader(validation_data, batch_size*2, num_workers, pin_memory=True)
testing_loader = DataLoader(testing_data, batch_size*2, num_workers, pin_memory=True)

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

# Lower: 64

batch_size = 64

training_loader = DataLoader(training_data, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
validation_loader = DataLoader(validation_data, batch_size*2, num_workers, pin_memory=True)
testing_loader = DataLoader(testing_data, batch_size*2, num_workers, pin_memory=True)

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

# Higher: 256

batch_size = 256

training_loader = DataLoader(training_data, batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)
validation_loader = DataLoader(validation_data, batch_size*2, num_workers, pin_memory=True)
testing_loader = DataLoader(testing_data, batch_size*2, num_workers, pin_memory=True)

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

"""Varying optimisation function: Stochastic Gradient Descent vs Adam"""

# Baseline: SGD

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

# Alternative optimiser function: Adam

optimiser_function = torch.optim.Adam

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

"""Varying learning rate: 0.1, 0.01, 0.001"""

# Baseline: 0.1

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

# 0.01

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.01 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

# 0.001

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

"""Varying number of hidden layers: Baseline (one hidden), two hidden, three hidden"""

# Baseline - One hidden

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

# Two hidden layers

class Model_two_hidden(NeuralNetwork):
    def __init__(self):

        super().__init__()
        self.linear1 = nn.Linear(input_size, 256)
        self.linear2 = nn.Linear(256, 128)
        self.linear3 = nn.Linear(128, 128)
        self.linear4 = nn.Linear(128, output_size)
        
    def forward(self, x):

        # Flatten images into vectors
        x = x.view(x.size(0), -1)

        # Apply layers & activation functions
        x = self.linear1(x)
        x = F.relu(x)
        x = self.linear2(x)
        x = F.relu(x)
        x = self.linear3(x)
        x = F.relu(x)
        x = self.linear4(x)
        
        return x

model = move_to_device(Model_two_hidden(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

class Model_three_hidden(NeuralNetwork):
    def __init__(self):

        super().__init__()
        self.linear1 = nn.Linear(input_size, 256)
        self.linear2 = nn.Linear(256, 256)
        self.linear3 = nn.Linear(256, 128)
        self.linear4 = nn.Linear(128, 128)
        self.linear5 = nn.Linear(128, output_size)
        
    def forward(self, x):

        # Flatten images into vectors
        x = x.view(x.size(0), -1)

        # Apply layers & activation functions
        x = self.linear1(x)
        x = F.relu(x)
        x = self.linear2(x)
        x = F.relu(x)
        x = self.linear3(x)
        
        return x

model = move_to_device(Model_three_hidden(), device) 

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

"""Varying number of nodes in hidden layers"""

# Baseline

batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 #1e-1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading

model = move_to_device(Model_1(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

# Fewer nodes

class Model_fewer_nodes(NeuralNetwork):
    def __init__(self):

        super().__init__()
        self.linear1 = nn.Linear(input_size, 128)
        self.linear2 = nn.Linear(128, 64)
        self.linear3 = nn.Linear(64, output_size)
        
    def forward(self, x):

        # Flatten images into vectors
        x = x.view(x.size(0), -1)

        # Apply layers & activation functions
        x = self.linear1(x)
        x = F.relu(x)
        x = self.linear2(x)
        x = F.relu(x)
        x = self.linear3(x)
        
        return x

model = move_to_device(Model_fewer_nodes(), device) 

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

# More nodes

class Model_more_nodes(NeuralNetwork):
    def __init__(self):

        super().__init__()
        self.linear1 = nn.Linear(input_size, 512)
        self.linear2 = nn.Linear(512, 256)
        self.linear3 = nn.Linear(256, output_size)
        
    def forward(self, x):

        # Flatten images into vectors
        x = x.view(x.size(0), -1)

        # Apply layers & activation functions
        x = self.linear1(x)
        x = F.relu(x)
        x = self.linear2(x)
        x = F.relu(x)
        x = self.linear3(x)
        
        return x

model = move_to_device(Model_more_nodes(), device) 

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

"""# Final Model

Combining the best performing parameters
"""

# Final parameters
batch_size = 128 # Number of images loaded into network at once
epochs = 10 # Training run time
optimiser_function = torch.optim.SGD # Optimiser function - set to Stochastic Gradient Descent
learning_rate = 0.1 # Rate of optimisation
num_workers = 2 # subprocesses to use for data loading
weight_decay = 1e-4 # L2 Regularisation parameter

training_loader = DeviceDataLoader(training_loader, device)
validation_loader = DeviceDataLoader(validation_loader, device)
testing_loader = DeviceDataLoader(testing_loader, device)

class Model_Final(NeuralNetwork):
    def __init__(self):

        super().__init__()
        self.linear1 = nn.Linear(input_size, 256) # Input layer
        self.linear2 = nn.Linear(256, 256) # Hidden layer
        self.linear3 = nn.Linear(256, 128) # Hidden layer
        self.linear4 = nn.Linear(128, output_size) # Output layer
        self.dropout = nn.Dropout(0.15) # Dropout layer: probability set to 0.25
        
    def forward(self, x):

        # Flatten images into vectors
        x = x.view(x.size(0), -1)

        # Apply layers & relu functions
        x = self.linear1(x)
        x = F.relu(x)
        x = self.linear2(x)
        x = F.relu(x)
        x = self.linear3(x)
        x = F.relu(x)
        x = self.dropout(x) # Applying dropout
        x = self.linear4(x)
        
        return x

model = move_to_device(Model_Final(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)

learning_rate = 0.01 #Reducing learning rate
history += fit_model_L2(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function, weight_decay) # Applying L2 regularisation

plot_losses(history)

plot_accuracies(history)

"""Repeating the final model with an inital learning rate of 0.01 instead of 0.1.  This is an attempt to smooth out the gradient"""

learning_rate = 0.01 # Update to final model

model = move_to_device(Model_Final(), device)

history = [validate_model(model, validation_loader)]
print(history)

history += fit_model(epochs, learning_rate, model, training_loader, validation_loader, optimiser_function)

plot_losses(history)

plot_accuracies(history)